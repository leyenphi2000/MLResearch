{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leyen\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3071: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "#Import packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "data1 = pd.read_csv(\"rao_nhek_10kb.tads.boundary.4mer.features.csv\")\n",
    "data2 = pd.read_csv(\"rao_imr90.tads.boundary.4mer.features.csv\")\n",
    "data3 = pd.read_csv(\"dixon_h1esc.tads.boundary.4mer.features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take out the unneeded row\n",
    "i = data1[(data1.AAAA == 'AAAA')].index\n",
    "data1 = data1.drop(i)\n",
    "\n",
    "i = data2[(data2.AAAA == 'AAAA')].index\n",
    "data2 = data2.drop(i)\n",
    "\n",
    "i = data3[(data3.AAAA == 'AAAA')].index\n",
    "data3 = data3.drop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Indicate features and class label\n",
    "x1 = data1.iloc[:,1:-1].apply(pd.to_numeric,errors='coerce')\n",
    "y1 = data1[\"class\"].astype('int')\n",
    "\n",
    "x2 = data2.iloc[:,1:-1].apply(pd.to_numeric,errors='coerce')\n",
    "y2 = data2[\"class\"].astype('int')\n",
    "\n",
    "x3 = data3.iloc[:,1:-1].apply(pd.to_numeric,errors='coerce')\n",
    "y3 = data3[\"class\"].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into training (80%) and testing (testing)\n",
    "x1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1, test_size=0.20, random_state=12)\n",
    "\n",
    "x2_train, x2_test, y2_train, y2_test = train_test_split(x2, y2, test_size=0.20, random_state=12)\n",
    "\n",
    "x3_train, x3_test, y3_train, y3_test = train_test_split(x3, y3, test_size=0.20, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the features to center zero mean and unit standard deviation\n",
    "ss = StandardScaler()\n",
    "x1_train = ss.fit_transform(x1_train)\n",
    "x1_test = ss.transform(x1_test)\n",
    "\n",
    "x2_train = ss.fit_transform(x2_train)\n",
    "x2_test = ss.transform(x2_test)\n",
    "\n",
    "x3_train = ss.fit_transform(x3_train)\n",
    "x3_test = ss.transform(x3_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Build the random forest model\n",
    "model1 = RandomForestClassifier()\n",
    "model1.fit(x1_train, y1_train)\n",
    "\n",
    "model2 = RandomForestClassifier()\n",
    "model2.fit(x2_train, y2_train)\n",
    "\n",
    "model3 = RandomForestClassifier()\n",
    "model3.fit(x3_train, y3_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest model with 10-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1:\n",
      "[0.59108527 0.60658915 0.59987072 0.58758888 0.57983193 0.60245637\n",
      " 0.58629606 0.60180995 0.59405301 0.59922431]\n",
      "[0.59689922 0.64341085 0.60981912 0.61498708 0.61757106 0.5994832\n",
      " 0.64341085 0.625323   0.59067358 0.61917098]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "train_score1 = cross_val_score(model1, x1_train, y1_train, scoring = 'accuracy', cv=10)\n",
    "test_score1 = cross_val_score(model1, x1_test, y1_test, scoring = 'accuracy', cv=10)\n",
    "\n",
    "print(\"Model 1:\")\n",
    "print(train_score1)\n",
    "print(test_score1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2:\n",
      "[0.61229508 0.60245902 0.63412633 0.62920427 0.62100082 0.62346185\n",
      " 0.61033634 0.61936013 0.62264151 0.609516  ]\n",
      "[0.60327869 0.59344262 0.66557377 0.60983607 0.62295082 0.56393443\n",
      " 0.60983607 0.56393443 0.62171053 0.53618421]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "train_score2 = cross_val_score(model2, x2_train, y2_train, scoring = 'accuracy', cv=10)\n",
    "test_score2 = cross_val_score(model2, x2_test, y2_test, scoring = 'accuracy', cv=10)\n",
    "\n",
    "print(\"Model 2:\")\n",
    "print(train_score2)\n",
    "print(test_score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3:\n",
      "[0.60431655 0.65467626 0.62829736 0.56971154 0.63942308 0.57692308\n",
      " 0.62259615 0.625      0.60817308 0.58413462]\n",
      "[0.65714286 0.58653846 0.61538462 0.55769231 0.58653846 0.64423077\n",
      " 0.61538462 0.59615385 0.56730769 0.66346154]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "train_score3 = cross_val_score(model3, x3_train, y3_train, scoring = 'accuracy', cv=10)\n",
    "test_score3 = cross_val_score(model3, x3_test, y3_test, scoring = 'accuracy', cv=10)\n",
    "\n",
    "print(\"Model 3:\")\n",
    "print(train_score3)\n",
    "print(test_score3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_pred = model1.predict(x1_test)\n",
    "y2_pred = model2.predict(x2_test)\n",
    "y3_pred = model3.predict(x3_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further tuning the modelâ€™s hyperparameters with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_param = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'bootstrap': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'bootstrap': False, 'criterion': 'gini', 'n_estimators': 150}\n",
      "Best Score:  0.6084798345398139\n"
     ]
    }
   ],
   "source": [
    "clf1 = GridSearchCV(model1, model_param, scoring='accuracy', cv=10)\n",
    "mod1 = clf1.fit(x1, y1)\n",
    "\n",
    "#Print out the the best parameter and score\n",
    "params = clf1.best_params_\n",
    "print(\"Best Parameters: \", params)\n",
    "\n",
    "score = clf1.best_score_\n",
    "print(\"Best Score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'bootstrap': True, 'criterion': 'entropy', 'n_estimators': 150}\n",
      "Best Score:  0.6210629921259843\n"
     ]
    }
   ],
   "source": [
    "clf2 = GridSearchCV(model2, model_param, scoring='accuracy', cv=10)\n",
    "mod2 = clf2.fit(x2, y2)\n",
    "\n",
    "#Print out the the best parameter and score\n",
    "params = clf2.best_params_\n",
    "print(\"Best Parameters: \", params)\n",
    "\n",
    "score = clf2.best_score_\n",
    "print(\"Best Score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'bootstrap': True, 'criterion': 'entropy', 'n_estimators': 150}\n",
      "Best Score:  0.6216270485752251\n"
     ]
    }
   ],
   "source": [
    "clf3 = GridSearchCV(model3, model_param, scoring='accuracy', cv=10)\n",
    "mod3 = clf3.fit(x3, y3)\n",
    "\n",
    "#Print out the the best parameter and score\n",
    "params = clf3.best_params_\n",
    "print(\"Best Parameters: \", params)\n",
    "\n",
    "score = clf3.best_score_\n",
    "print(\"Best Score: \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy, AUC (area under the curve) score, Precision, Recall, F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1:\n",
      "Accuracy:  0.6080661840744571\n",
      "AUC:  0.6084059953085404\n",
      "Precision:  0.6235480464625132\n",
      "Recall:  0.5952620967741935\n",
      "F1 score:  0.6090768437338834\n",
      "Model 2:\n",
      "Accuracy:  0.6128608923884514\n",
      "AUC:  0.6128205846059089\n",
      "Precision:  0.6042356055592323\n",
      "Recall:  0.6107023411371237\n",
      "F1 score:  0.607451763140386\n",
      "Model 3:\n",
      "Accuracy:  0.5936599423631124\n",
      "AUC:  0.5936925564192855\n",
      "Precision:  0.5972495088408645\n",
      "Recall:  0.5823754789272031\n",
      "F1 score:  0.5897187196896218\n"
     ]
    }
   ],
   "source": [
    "#Accuracy, Precision, Recall, F1 score\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"Model 1:\")\n",
    "print(\"Accuracy: \", accuracy_score(y1_test, y1_pred))\n",
    "print(\"AUC: \", roc_auc_score(y1_test, y1_pred))\n",
    "print(\"Precision: \", precision_score(y1_test, y1_pred))\n",
    "print(\"Recall: \", recall_score(y1_test, y1_pred))\n",
    "print(\"F1 score: \", f1_score(y1_test, y1_pred))\n",
    "\n",
    "print(\"Model 2:\")\n",
    "print(\"Accuracy: \", accuracy_score(y2_test, y2_pred))\n",
    "print(\"AUC: \", roc_auc_score(y2_test, y2_pred))\n",
    "print(\"Precision: \", precision_score(y2_test, y2_pred))\n",
    "print(\"Recall: \", recall_score(y2_test, y2_pred))\n",
    "print(\"F1 score: \", f1_score(y2_test, y2_pred))\n",
    "\n",
    "print(\"Model 3:\")\n",
    "print(\"Accuracy: \", accuracy_score(y3_test, y3_pred))\n",
    "print(\"AUC: \", roc_auc_score(y3_test, y3_pred))\n",
    "print(\"Precision: \", precision_score(y3_test, y3_pred))\n",
    "print(\"Recall: \", recall_score(y3_test, y3_pred))\n",
    "print(\"F1 score: \", f1_score(y3_test, y3_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the trained model to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "filename1 = 'rao_nhek.pickle'\n",
    "pickle.dump(model1, open(filename1, 'wb'))\n",
    "\n",
    "filename2 = 'rao_imr90.pickle'\n",
    "pickle.dump(model2, open(filename2, 'wb'))\n",
    "\n",
    "filename3 = 'dixon_h1esc.pickle'\n",
    "pickle.dump(model3, open(filename3, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
